{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: Deep Q-Network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 01: Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#states =  (4,)\n",
      "#actions =  2\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import gym \n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prepare environment\n",
    "env = gym.make(\"CartPole-v1\").env\n",
    "env.reset()\n",
    "# number of states\n",
    "state_dim = env.observation_space.shape\n",
    "print(\"#states = \", state_dim)\n",
    "\n",
    "# number of actions\n",
    "n_actions = env.action_space.n \n",
    "print(\"#actions = \", n_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 02: Create Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "# define class of Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim[0], 64)\n",
    "        self.fc2 = nn.Linear(64, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, n_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)        \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def predict(self, state):\n",
    "        # convert state variable to torch tensor\n",
    "        state = torch.tensor(state, dtype = torch.float32)\n",
    "        state = state.unsqueeze(0)  # add batch dimension\n",
    "        # make prediction\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state)\n",
    "\n",
    "        return q_values.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize q-network\n",
    "qnet = QNetwork(state_dim, n_actions)\n",
    "qnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state =  [ 0.00567684 -0.0295081  -0.01180258 -0.03759454]\n",
      "Predicted q-values =  [-0.10537921 -0.13273744]\n"
     ]
    }
   ],
   "source": [
    "# take the initial state\n",
    "s = env.reset()\n",
    "print(\"initial state = \", s[0])\n",
    "\n",
    "# test prediction for initial state\n",
    "example_state = s\n",
    "example_pred = qnet.predict(example_state[0])\n",
    "print(\"Predicted q-values = \", example_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take action with epsilon-greedy policy\n",
    "\n",
    "def get_action(state, epsilon=0):\n",
    "    # calculate q-value\n",
    "    q_values = qnet.predict(state)\n",
    "\n",
    "    # if random parameter is better, take random action\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice(len(q_values))\n",
    "    # else, take the best action related with the highest Q(s, a)\n",
    "    else:\n",
    "        action = np.argmax(q_values)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing current values in environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take the action for initial state:  0\n",
      "current state =  [ 0.00567684 -0.0295081  -0.01180258 -0.03759454]\n",
      "action =  0\n",
      "next state =  [ 0.00508668 -0.22445883 -0.01255447  0.2513413 ]\n",
      "reward =  1.0\n",
      "is final state =  False\n"
     ]
    }
   ],
   "source": [
    "# test the function get action\n",
    "a_test = get_action(s[0], epsilon = 0)\n",
    "print(\"take the action for initial state: \", a_test) \n",
    "\n",
    "# current state\n",
    "current_state = s[0]\n",
    "print(\"current state = \", current_state)\n",
    "\n",
    "# action \n",
    "print(\"action = \", a_test)\n",
    "\n",
    "# next state\n",
    "next_state, r, done, _, _ = env.step(a_test)\n",
    "print(\"next state = \", next_state)\n",
    "\n",
    "# reward\n",
    "print(\"reward = \", r)\n",
    "\n",
    "# final state?\n",
    "print(\"is final state = \", done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert environment variables to tensor\n",
    "def convert_env_to_tensor(s, a, r, next_s, done):\n",
    "    s = torch.as_tensor(s, dtype = torch.float32).unsqueeze(0)\n",
    "    a = torch.as_tensor(a, dtype = torch.int64).unsqueeze(0)\n",
    "    r = torch.as_tensor(r, dtype = torch.float32).unsqueeze(0)\n",
    "    next_s = torch.as_tensor(next_s, dtype = torch.float32).unsqueeze(0)\n",
    "    done = torch.as_tensor(done, dtype = torch.bool).unsqueeze(0)\n",
    "\n",
    "    return s, a, r, next_s, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test conversion of variables to tensor\n",
    "current_state, a_test, r, next_state, done = convert_env_to_tensor(current_state, a_test, r, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0057, -0.0295, -0.0118, -0.0376]])\n",
      "tensor([[-0.1054, -0.1327]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0051, -0.2245, -0.0126,  0.2513]])\n",
      "tensor([[-0.1096, -0.1324]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# current state and this prediction\n",
    "print(current_state)\n",
    "print(qnet(current_state))\n",
    "\n",
    "# next-state and this prediction\n",
    "print(next_state)\n",
    "print(qnet(next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "assert isinstance(qnet.fc3, torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon=0.0 tests passed\n",
      "epsilon=0.1 tests passed\n",
      "epsilon=0.5 tests passed\n",
      "epsilon=1.0 tests passed\n"
     ]
    }
   ],
   "source": [
    "# test epsilon-greedy condition\n",
    "s = env.reset()\n",
    "s = s[0]\n",
    "\n",
    "for eps in [0.0, 0.1, 0.5, 1.0]:\n",
    "    state_freq = np.zeros(n_actions, dtype = int)\n",
    "\n",
    "    for i in range(10000):\n",
    "        action = get_action(s, epsilon = eps)\n",
    "        state_freq[action] += 1\n",
    "\n",
    "    best_action = np.argmax(state_freq)\n",
    "    \n",
    "    assert abs(state_freq[best_action] - 10000 * (1 - eps + eps/n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_freq[other_action] - 10000 * (eps/n_actions)) < 200\n",
    "            \n",
    "    print('epsilon=%.1f tests passed'%eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 03: Integrate Q-Learning with Gradient Descendent"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function:  \n",
    "  \n",
    "![image.png](attachment:image.png)\n",
    "  \n",
    "Where:  \n",
    "s: current state  \n",
    "a: action  \n",
    "r: reward  \n",
    "s': next state  \n",
    "gamma: discount factor  \n",
    "Q_theta(s, a): result of Q-value given chosed politic by neural network  \n",
    "Q-: max{a}{Q(s', a')}: is the max Q-value for the next state s_t+1 over all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension to testing with random values\n",
    "states_ph = torch.empty((1,) + state_dim, dtype=torch.float32)  \n",
    "actions_ph = torch.empty((1,), dtype=torch.int64) \n",
    "rewards_ph = torch.empty((1,), dtype=torch.float32)\n",
    "next_states_ph = torch.empty((1,) + state_dim, dtype=torch.float32)\n",
    "is_done_ph = torch.empty((1,), dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1032], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# setting q-values for all actions in the current state\n",
    "# calculate Q_theta(s, a) from Q-network\n",
    "pred_qvalues = qnet(states_ph)\n",
    "\n",
    "# clip actions_ph to stay between [0, n_actions -1]\n",
    "actions_ph = torch.clamp(actions_ph, 0, n_actions-1)\n",
    "\n",
    "# one-hot encode for actions_ph tensor\n",
    "actions_oh = torch.nn.functional.one_hot(actions_ph, n_actions).to(torch.float32)\n",
    "\n",
    "# calculate predicted q-values for actions\n",
    "pred_qvalues_x_actions = torch.sum(pred_qvalues * actions_oh, dim = 1)\n",
    "print(pred_qvalues_x_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial states =  tensor([[-0.1059, -0.1320, -0.0018,  0.0150]])\n",
      "predicted q-value =  tensor([[-0.1032, -0.1323]], grad_fn=<AddmmBackward0>)\n",
      "next states =  tensor([[-0.0050, -0.0410, -0.0018,  0.0150]])\n",
      "predicted q-value =  tensor([[-0.1059, -0.1320]], grad_fn=<AddmmBackward0>)\n",
      "actions =  tensor([0])\n"
     ]
    }
   ],
   "source": [
    "print(\"initial states = \", states_ph)\n",
    "print(\"predicted q-value = \", qnet(states_ph))\n",
    "print(\"next states = \", next_states_ph)\n",
    "print(\"predicted q-value = \", qnet(next_states_ph))\n",
    "print(\"actions = \", actions_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2107], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# apply NN to obtain Q-values for next states\n",
    "pred_next_qvalues = qnet(next_states_ph)\n",
    "\n",
    "# calculate V*(next_states) using the predicted next Q-values \n",
    "# V*(s') = max{a}Q*(s, a)\n",
    "next_state_values, _ = torch.max(pred_next_qvalues, dim = 1)\n",
    "\n",
    "# calculate formula of loss function\n",
    "# loss = 1/N * sum{( Q(si, ai) - [ri + gamma * max{a'}Q(s_i+1, a')] )**2}\n",
    "# where target_q-value = ri + gamma * max{a'}Q(s_i+1, a')\n",
    "target_qvalues_x_actions = rewards_ph + gamma * next_state_values\n",
    "print(target_qvalues_x_actions)\n",
    "\n",
    "# in the last state use the formula Q(s, a) = r(s, a), \n",
    "# given the fact that don't exist next state\n",
    "target_qvalues_x_actions = torch.where(is_done_ph, rewards_ph, target_qvalues_x_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define MSE-loss function\n",
    "loss = (pred_qvalues_x_actions - target_qvalues_x_actions.detach())**2\n",
    "loss = torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0116, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# check the loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 04: Playing with Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer algorithm\n",
    "import torch.optim as optim \n",
    "\n",
    "optimizer = optim.Adam(qnet.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(env, t_max = 1000, epsilon = 0, train = False):\n",
    "\n",
    "    # define the total reward\n",
    "    total_reward = 0\n",
    "\n",
    "    # take the initial state\n",
    "    s = env.reset()    \n",
    "    s  = s[0]\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # take the action\n",
    "        a = get_action(s, epsilon)\n",
    "        \n",
    "        # check the next step\n",
    "        next_s, r, done, _, _ = env.step(a)\n",
    "\n",
    "        if train:            \n",
    "            # convert all env-variables to torch tensor\n",
    "            s, a, r, next_s, done = convert_env_to_tensor(s, a, r, next_s, done)\n",
    "\n",
    "            # calculate Q_theta\n",
    "            pred_qvalues = qnet(s)                    \n",
    "            actions_ph = torch.clamp(a, 0, n_actions-1)\n",
    "            actions_oh = torch.nn.functional.one_hot(actions_ph, n_actions).to(torch.float32)                \n",
    "            pred_qvalues_x_actions = torch.sum(pred_qvalues * actions_oh, dim = 1)\n",
    "            \n",
    "            # calculate Q-target\n",
    "            pred_next_qvalues = qnet(next_s)            \n",
    "            next_state_values, _ = torch.max(pred_next_qvalues, dim = 1)\n",
    "            target_qvalues_x_actions = r + gamma * next_state_values\n",
    "            target_qvalues_x_actions = torch.where(done, r, target_qvalues_x_actions)\n",
    "            loss = torch.mean((pred_qvalues_x_actions - target_qvalues_x_actions.detach())**2)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # accumulate the reward\n",
    "        total_reward += r\n",
    "\n",
    "        # advance to next state\n",
    "        s = next_s\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13668\\635027586.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 9.000\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 14.000\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 12.000\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 9.000\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 10.000\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 10.000\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 10.000\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 11.000\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 12.000\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 14.000\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 13.000\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 12.000\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 17.000\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 10.000\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 13.000\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 9.000\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 19.000\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 10.000\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 10.000\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 11.000\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 11.000\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 12.000\tepsilon = 0.405\n",
      "epoch #22\tmean reward = 15.000\tepsilon = 0.401\n",
      "epoch #23\tmean reward = 11.000\tepsilon = 0.397\n",
      "epoch #24\tmean reward = 11.000\tepsilon = 0.393\n",
      "epoch #25\tmean reward = 12.000\tepsilon = 0.389\n",
      "epoch #26\tmean reward = 12.000\tepsilon = 0.385\n",
      "epoch #27\tmean reward = 12.000\tepsilon = 0.381\n",
      "epoch #28\tmean reward = 13.000\tepsilon = 0.377\n",
      "epoch #29\tmean reward = 10.000\tepsilon = 0.374\n",
      "epoch #30\tmean reward = 15.000\tepsilon = 0.370\n",
      "epoch #31\tmean reward = 10.000\tepsilon = 0.366\n",
      "epoch #32\tmean reward = 9.000\tepsilon = 0.362\n",
      "epoch #33\tmean reward = 17.000\tepsilon = 0.359\n",
      "epoch #34\tmean reward = 18.000\tepsilon = 0.355\n",
      "epoch #35\tmean reward = 23.000\tepsilon = 0.352\n",
      "epoch #36\tmean reward = 10.000\tepsilon = 0.348\n",
      "epoch #37\tmean reward = 10.000\tepsilon = 0.345\n",
      "epoch #38\tmean reward = 10.000\tepsilon = 0.341\n",
      "epoch #39\tmean reward = 9.000\tepsilon = 0.338\n",
      "epoch #40\tmean reward = 12.000\tepsilon = 0.334\n",
      "epoch #41\tmean reward = 12.000\tepsilon = 0.331\n",
      "epoch #42\tmean reward = 11.000\tepsilon = 0.328\n",
      "epoch #43\tmean reward = 8.000\tepsilon = 0.325\n",
      "epoch #44\tmean reward = 9.000\tepsilon = 0.321\n",
      "epoch #45\tmean reward = 14.000\tepsilon = 0.318\n",
      "epoch #46\tmean reward = 15.000\tepsilon = 0.315\n",
      "epoch #47\tmean reward = 26.000\tepsilon = 0.312\n",
      "epoch #48\tmean reward = 14.000\tepsilon = 0.309\n",
      "epoch #49\tmean reward = 13.000\tepsilon = 0.306\n",
      "epoch #50\tmean reward = 9.000\tepsilon = 0.303\n",
      "epoch #51\tmean reward = 10.000\tepsilon = 0.299\n",
      "epoch #52\tmean reward = 14.000\tepsilon = 0.296\n",
      "epoch #53\tmean reward = 11.000\tepsilon = 0.294\n",
      "epoch #54\tmean reward = 10.000\tepsilon = 0.291\n",
      "epoch #55\tmean reward = 12.000\tepsilon = 0.288\n",
      "epoch #56\tmean reward = 10.000\tepsilon = 0.285\n",
      "epoch #57\tmean reward = 10.000\tepsilon = 0.282\n",
      "epoch #58\tmean reward = 10.000\tepsilon = 0.279\n",
      "epoch #59\tmean reward = 11.000\tepsilon = 0.276\n",
      "epoch #60\tmean reward = 13.000\tepsilon = 0.274\n",
      "epoch #61\tmean reward = 11.000\tepsilon = 0.271\n",
      "epoch #62\tmean reward = 16.000\tepsilon = 0.268\n",
      "epoch #63\tmean reward = 11.000\tepsilon = 0.265\n",
      "epoch #64\tmean reward = 9.000\tepsilon = 0.263\n",
      "epoch #65\tmean reward = 10.000\tepsilon = 0.260\n",
      "epoch #66\tmean reward = 14.000\tepsilon = 0.258\n",
      "epoch #67\tmean reward = 10.000\tepsilon = 0.255\n",
      "epoch #68\tmean reward = 9.000\tepsilon = 0.252\n",
      "epoch #69\tmean reward = 11.000\tepsilon = 0.250\n",
      "epoch #70\tmean reward = 16.000\tepsilon = 0.247\n",
      "epoch #71\tmean reward = 13.000\tepsilon = 0.245\n",
      "epoch #72\tmean reward = 22.000\tepsilon = 0.242\n",
      "epoch #73\tmean reward = 17.000\tepsilon = 0.240\n",
      "epoch #74\tmean reward = 58.000\tepsilon = 0.238\n",
      "epoch #75\tmean reward = 14.000\tepsilon = 0.235\n",
      "epoch #76\tmean reward = 14.000\tepsilon = 0.233\n",
      "epoch #77\tmean reward = 9.000\tepsilon = 0.231\n",
      "epoch #78\tmean reward = 11.000\tepsilon = 0.228\n",
      "epoch #79\tmean reward = 8.000\tepsilon = 0.226\n",
      "epoch #80\tmean reward = 10.000\tepsilon = 0.224\n",
      "epoch #81\tmean reward = 10.000\tepsilon = 0.222\n",
      "epoch #82\tmean reward = 10.000\tepsilon = 0.219\n",
      "epoch #83\tmean reward = 10.000\tepsilon = 0.217\n",
      "epoch #84\tmean reward = 13.000\tepsilon = 0.215\n",
      "epoch #85\tmean reward = 10.000\tepsilon = 0.213\n",
      "epoch #86\tmean reward = 12.000\tepsilon = 0.211\n",
      "epoch #87\tmean reward = 12.000\tepsilon = 0.209\n",
      "epoch #88\tmean reward = 9.000\tepsilon = 0.206\n",
      "epoch #89\tmean reward = 10.000\tepsilon = 0.204\n",
      "epoch #90\tmean reward = 11.000\tepsilon = 0.202\n",
      "epoch #91\tmean reward = 10.000\tepsilon = 0.200\n",
      "epoch #92\tmean reward = 10.000\tepsilon = 0.198\n",
      "epoch #93\tmean reward = 13.000\tepsilon = 0.196\n",
      "epoch #94\tmean reward = 13.000\tepsilon = 0.194\n",
      "epoch #95\tmean reward = 11.000\tepsilon = 0.192\n",
      "epoch #96\tmean reward = 10.000\tepsilon = 0.191\n",
      "epoch #97\tmean reward = 10.000\tepsilon = 0.189\n",
      "epoch #98\tmean reward = 11.000\tepsilon = 0.187\n",
      "epoch #99\tmean reward = 9.000\tepsilon = 0.185\n",
      "epoch #100\tmean reward = 10.000\tepsilon = 0.183\n",
      "epoch #101\tmean reward = 11.000\tepsilon = 0.181\n",
      "epoch #102\tmean reward = 16.000\tepsilon = 0.179\n",
      "epoch #103\tmean reward = 12.000\tepsilon = 0.178\n",
      "epoch #104\tmean reward = 9.000\tepsilon = 0.176\n",
      "epoch #105\tmean reward = 14.000\tepsilon = 0.174\n",
      "epoch #106\tmean reward = 12.000\tepsilon = 0.172\n",
      "epoch #107\tmean reward = 11.000\tepsilon = 0.171\n",
      "epoch #108\tmean reward = 11.000\tepsilon = 0.169\n",
      "epoch #109\tmean reward = 8.000\tepsilon = 0.167\n",
      "epoch #110\tmean reward = 11.000\tepsilon = 0.166\n",
      "epoch #111\tmean reward = 10.000\tepsilon = 0.164\n",
      "epoch #112\tmean reward = 10.000\tepsilon = 0.162\n",
      "epoch #113\tmean reward = 10.000\tepsilon = 0.161\n",
      "epoch #114\tmean reward = 13.000\tepsilon = 0.159\n",
      "epoch #115\tmean reward = 10.000\tepsilon = 0.157\n",
      "epoch #116\tmean reward = 16.000\tepsilon = 0.156\n",
      "epoch #117\tmean reward = 11.000\tepsilon = 0.154\n",
      "epoch #118\tmean reward = 10.000\tepsilon = 0.153\n",
      "epoch #119\tmean reward = 10.000\tepsilon = 0.151\n",
      "epoch #120\tmean reward = 10.000\tepsilon = 0.150\n",
      "epoch #121\tmean reward = 12.000\tepsilon = 0.148\n",
      "epoch #122\tmean reward = 12.000\tepsilon = 0.147\n",
      "epoch #123\tmean reward = 8.000\tepsilon = 0.145\n",
      "epoch #124\tmean reward = 11.000\tepsilon = 0.144\n",
      "epoch #125\tmean reward = 10.000\tepsilon = 0.142\n",
      "epoch #126\tmean reward = 14.000\tepsilon = 0.141\n",
      "epoch #127\tmean reward = 10.000\tepsilon = 0.140\n",
      "epoch #128\tmean reward = 8.000\tepsilon = 0.138\n",
      "epoch #129\tmean reward = 12.000\tepsilon = 0.137\n",
      "epoch #130\tmean reward = 10.000\tepsilon = 0.135\n",
      "epoch #131\tmean reward = 11.000\tepsilon = 0.134\n",
      "epoch #132\tmean reward = 10.000\tepsilon = 0.133\n",
      "epoch #133\tmean reward = 9.000\tepsilon = 0.131\n",
      "epoch #134\tmean reward = 14.000\tepsilon = 0.130\n",
      "epoch #135\tmean reward = 14.000\tepsilon = 0.129\n",
      "epoch #136\tmean reward = 19.000\tepsilon = 0.127\n",
      "epoch #137\tmean reward = 33.000\tepsilon = 0.126\n",
      "epoch #138\tmean reward = 13.000\tepsilon = 0.125\n",
      "epoch #139\tmean reward = 9.000\tepsilon = 0.124\n",
      "epoch #140\tmean reward = 9.000\tepsilon = 0.122\n",
      "epoch #141\tmean reward = 10.000\tepsilon = 0.121\n",
      "epoch #142\tmean reward = 9.000\tepsilon = 0.120\n",
      "epoch #143\tmean reward = 11.000\tepsilon = 0.119\n",
      "epoch #144\tmean reward = 8.000\tepsilon = 0.118\n",
      "epoch #145\tmean reward = 10.000\tepsilon = 0.116\n",
      "epoch #146\tmean reward = 10.000\tepsilon = 0.115\n",
      "epoch #147\tmean reward = 10.000\tepsilon = 0.114\n",
      "epoch #148\tmean reward = 10.000\tepsilon = 0.113\n",
      "epoch #149\tmean reward = 9.000\tepsilon = 0.112\n",
      "epoch #150\tmean reward = 10.000\tepsilon = 0.111\n",
      "epoch #151\tmean reward = 10.000\tepsilon = 0.110\n",
      "epoch #152\tmean reward = 10.000\tepsilon = 0.109\n",
      "epoch #153\tmean reward = 9.000\tepsilon = 0.107\n",
      "epoch #154\tmean reward = 10.000\tepsilon = 0.106\n",
      "epoch #155\tmean reward = 11.000\tepsilon = 0.105\n",
      "epoch #156\tmean reward = 9.000\tepsilon = 0.104\n",
      "epoch #157\tmean reward = 9.000\tepsilon = 0.103\n",
      "epoch #158\tmean reward = 9.000\tepsilon = 0.102\n",
      "epoch #159\tmean reward = 9.000\tepsilon = 0.101\n",
      "epoch #160\tmean reward = 9.000\tepsilon = 0.100\n",
      "epoch #161\tmean reward = 8.000\tepsilon = 0.099\n",
      "epoch #162\tmean reward = 11.000\tepsilon = 0.098\n",
      "epoch #163\tmean reward = 11.000\tepsilon = 0.097\n",
      "epoch #164\tmean reward = 9.000\tepsilon = 0.096\n",
      "epoch #165\tmean reward = 10.000\tepsilon = 0.095\n",
      "epoch #166\tmean reward = 9.000\tepsilon = 0.094\n",
      "epoch #167\tmean reward = 11.000\tepsilon = 0.093\n",
      "epoch #168\tmean reward = 10.000\tepsilon = 0.092\n",
      "epoch #169\tmean reward = 10.000\tepsilon = 0.091\n",
      "epoch #170\tmean reward = 11.000\tepsilon = 0.091\n",
      "epoch #171\tmean reward = 9.000\tepsilon = 0.090\n",
      "epoch #172\tmean reward = 12.000\tepsilon = 0.089\n",
      "epoch #173\tmean reward = 15.000\tepsilon = 0.088\n",
      "epoch #174\tmean reward = 14.000\tepsilon = 0.087\n",
      "epoch #175\tmean reward = 16.000\tepsilon = 0.086\n",
      "epoch #176\tmean reward = 14.000\tepsilon = 0.085\n",
      "epoch #177\tmean reward = 18.000\tepsilon = 0.084\n",
      "epoch #178\tmean reward = 31.000\tepsilon = 0.084\n",
      "epoch #179\tmean reward = 10.000\tepsilon = 0.083\n",
      "epoch #180\tmean reward = 10.000\tepsilon = 0.082\n",
      "epoch #181\tmean reward = 9.000\tepsilon = 0.081\n",
      "epoch #182\tmean reward = 8.000\tepsilon = 0.080\n",
      "epoch #183\tmean reward = 10.000\tepsilon = 0.079\n",
      "epoch #184\tmean reward = 10.000\tepsilon = 0.079\n",
      "epoch #185\tmean reward = 8.000\tepsilon = 0.078\n",
      "epoch #186\tmean reward = 10.000\tepsilon = 0.077\n",
      "epoch #187\tmean reward = 10.000\tepsilon = 0.076\n",
      "epoch #188\tmean reward = 10.000\tepsilon = 0.076\n",
      "epoch #189\tmean reward = 9.000\tepsilon = 0.075\n",
      "epoch #190\tmean reward = 10.000\tepsilon = 0.074\n",
      "epoch #191\tmean reward = 10.000\tepsilon = 0.073\n",
      "epoch #192\tmean reward = 11.000\tepsilon = 0.073\n",
      "epoch #193\tmean reward = 11.000\tepsilon = 0.072\n",
      "epoch #194\tmean reward = 10.000\tepsilon = 0.071\n",
      "epoch #195\tmean reward = 11.000\tepsilon = 0.070\n",
      "epoch #196\tmean reward = 10.000\tepsilon = 0.070\n",
      "epoch #197\tmean reward = 10.000\tepsilon = 0.069\n",
      "epoch #198\tmean reward = 11.000\tepsilon = 0.068\n",
      "epoch #199\tmean reward = 10.000\tepsilon = 0.068\n",
      "epoch #200\tmean reward = 8.000\tepsilon = 0.067\n",
      "epoch #201\tmean reward = 10.000\tepsilon = 0.066\n",
      "epoch #202\tmean reward = 12.000\tepsilon = 0.066\n",
      "epoch #203\tmean reward = 10.000\tepsilon = 0.065\n",
      "epoch #204\tmean reward = 8.000\tepsilon = 0.064\n",
      "epoch #205\tmean reward = 8.000\tepsilon = 0.064\n",
      "epoch #206\tmean reward = 10.000\tepsilon = 0.063\n",
      "epoch #207\tmean reward = 9.000\tepsilon = 0.062\n",
      "epoch #208\tmean reward = 10.000\tepsilon = 0.062\n",
      "epoch #209\tmean reward = 10.000\tepsilon = 0.061\n",
      "epoch #210\tmean reward = 9.000\tepsilon = 0.061\n",
      "epoch #211\tmean reward = 10.000\tepsilon = 0.060\n",
      "epoch #212\tmean reward = 11.000\tepsilon = 0.059\n",
      "epoch #213\tmean reward = 11.000\tepsilon = 0.059\n",
      "epoch #214\tmean reward = 11.000\tepsilon = 0.058\n",
      "epoch #215\tmean reward = 14.000\tepsilon = 0.058\n",
      "epoch #216\tmean reward = 11.000\tepsilon = 0.057\n",
      "epoch #217\tmean reward = 10.000\tepsilon = 0.056\n",
      "epoch #218\tmean reward = 10.000\tepsilon = 0.056\n",
      "epoch #219\tmean reward = 9.000\tepsilon = 0.055\n",
      "epoch #220\tmean reward = 10.000\tepsilon = 0.055\n",
      "epoch #221\tmean reward = 9.000\tepsilon = 0.054\n",
      "epoch #222\tmean reward = 10.000\tepsilon = 0.054\n",
      "epoch #223\tmean reward = 11.000\tepsilon = 0.053\n",
      "epoch #224\tmean reward = 9.000\tepsilon = 0.053\n",
      "epoch #225\tmean reward = 9.000\tepsilon = 0.052\n",
      "epoch #226\tmean reward = 10.000\tepsilon = 0.052\n",
      "epoch #227\tmean reward = 9.000\tepsilon = 0.051\n",
      "epoch #228\tmean reward = 10.000\tepsilon = 0.051\n",
      "epoch #229\tmean reward = 9.000\tepsilon = 0.050\n",
      "epoch #230\tmean reward = 10.000\tepsilon = 0.050\n",
      "epoch #231\tmean reward = 10.000\tepsilon = 0.049\n",
      "epoch #232\tmean reward = 12.000\tepsilon = 0.049\n",
      "epoch #233\tmean reward = 8.000\tepsilon = 0.048\n",
      "epoch #234\tmean reward = 9.000\tepsilon = 0.048\n",
      "epoch #235\tmean reward = 10.000\tepsilon = 0.047\n",
      "epoch #236\tmean reward = 10.000\tepsilon = 0.047\n",
      "epoch #237\tmean reward = 10.000\tepsilon = 0.046\n",
      "epoch #238\tmean reward = 9.000\tepsilon = 0.046\n",
      "epoch #239\tmean reward = 9.000\tepsilon = 0.045\n",
      "epoch #240\tmean reward = 11.000\tepsilon = 0.045\n",
      "epoch #241\tmean reward = 9.000\tepsilon = 0.044\n",
      "epoch #242\tmean reward = 8.000\tepsilon = 0.044\n",
      "epoch #243\tmean reward = 10.000\tepsilon = 0.043\n",
      "epoch #244\tmean reward = 10.000\tepsilon = 0.043\n",
      "epoch #245\tmean reward = 10.000\tepsilon = 0.043\n",
      "epoch #246\tmean reward = 10.000\tepsilon = 0.042\n",
      "epoch #247\tmean reward = 9.000\tepsilon = 0.042\n",
      "epoch #248\tmean reward = 10.000\tepsilon = 0.041\n",
      "epoch #249\tmean reward = 8.000\tepsilon = 0.041\n",
      "epoch #250\tmean reward = 9.000\tepsilon = 0.041\n",
      "epoch #251\tmean reward = 10.000\tepsilon = 0.040\n",
      "epoch #252\tmean reward = 9.000\tepsilon = 0.040\n",
      "epoch #253\tmean reward = 10.000\tepsilon = 0.039\n",
      "epoch #254\tmean reward = 10.000\tepsilon = 0.039\n",
      "epoch #255\tmean reward = 9.000\tepsilon = 0.039\n",
      "epoch #256\tmean reward = 10.000\tepsilon = 0.038\n",
      "epoch #257\tmean reward = 13.000\tepsilon = 0.038\n",
      "epoch #258\tmean reward = 10.000\tepsilon = 0.037\n",
      "epoch #259\tmean reward = 10.000\tepsilon = 0.037\n",
      "epoch #260\tmean reward = 9.000\tepsilon = 0.037\n",
      "epoch #261\tmean reward = 11.000\tepsilon = 0.036\n",
      "epoch #262\tmean reward = 12.000\tepsilon = 0.036\n",
      "epoch #263\tmean reward = 9.000\tepsilon = 0.036\n",
      "epoch #264\tmean reward = 9.000\tepsilon = 0.035\n",
      "epoch #265\tmean reward = 9.000\tepsilon = 0.035\n",
      "epoch #266\tmean reward = 9.000\tepsilon = 0.035\n",
      "epoch #267\tmean reward = 9.000\tepsilon = 0.034\n",
      "epoch #268\tmean reward = 9.000\tepsilon = 0.034\n",
      "epoch #269\tmean reward = 8.000\tepsilon = 0.033\n",
      "epoch #270\tmean reward = 10.000\tepsilon = 0.033\n",
      "epoch #271\tmean reward = 10.000\tepsilon = 0.033\n",
      "epoch #272\tmean reward = 9.000\tepsilon = 0.032\n",
      "epoch #273\tmean reward = 10.000\tepsilon = 0.032\n",
      "epoch #274\tmean reward = 9.000\tepsilon = 0.032\n",
      "epoch #275\tmean reward = 9.000\tepsilon = 0.032\n",
      "epoch #276\tmean reward = 10.000\tepsilon = 0.031\n",
      "epoch #277\tmean reward = 8.000\tepsilon = 0.031\n",
      "epoch #278\tmean reward = 9.000\tepsilon = 0.031\n",
      "epoch #279\tmean reward = 8.000\tepsilon = 0.030\n",
      "epoch #280\tmean reward = 9.000\tepsilon = 0.030\n",
      "epoch #281\tmean reward = 10.000\tepsilon = 0.030\n",
      "epoch #282\tmean reward = 9.000\tepsilon = 0.029\n",
      "epoch #283\tmean reward = 10.000\tepsilon = 0.029\n",
      "epoch #284\tmean reward = 9.000\tepsilon = 0.029\n",
      "epoch #285\tmean reward = 10.000\tepsilon = 0.029\n",
      "epoch #286\tmean reward = 10.000\tepsilon = 0.028\n",
      "epoch #287\tmean reward = 12.000\tepsilon = 0.028\n",
      "epoch #288\tmean reward = 10.000\tepsilon = 0.028\n",
      "epoch #289\tmean reward = 10.000\tepsilon = 0.027\n",
      "epoch #290\tmean reward = 10.000\tepsilon = 0.027\n",
      "epoch #291\tmean reward = 10.000\tepsilon = 0.027\n",
      "epoch #292\tmean reward = 11.000\tepsilon = 0.027\n",
      "epoch #293\tmean reward = 9.000\tepsilon = 0.026\n",
      "epoch #294\tmean reward = 11.000\tepsilon = 0.026\n",
      "epoch #295\tmean reward = 10.000\tepsilon = 0.026\n",
      "epoch #296\tmean reward = 9.000\tepsilon = 0.026\n",
      "epoch #297\tmean reward = 11.000\tepsilon = 0.025\n",
      "epoch #298\tmean reward = 10.000\tepsilon = 0.025\n",
      "epoch #299\tmean reward = 12.000\tepsilon = 0.025\n",
      "epoch #300\tmean reward = 10.000\tepsilon = 0.025\n",
      "epoch #301\tmean reward = 11.000\tepsilon = 0.024\n",
      "epoch #302\tmean reward = 9.000\tepsilon = 0.024\n",
      "epoch #303\tmean reward = 9.000\tepsilon = 0.024\n",
      "epoch #304\tmean reward = 9.000\tepsilon = 0.024\n",
      "epoch #305\tmean reward = 12.000\tepsilon = 0.023\n",
      "epoch #306\tmean reward = 11.000\tepsilon = 0.023\n",
      "epoch #307\tmean reward = 12.000\tepsilon = 0.023\n",
      "epoch #308\tmean reward = 11.000\tepsilon = 0.023\n",
      "epoch #309\tmean reward = 14.000\tepsilon = 0.022\n",
      "epoch #310\tmean reward = 17.000\tepsilon = 0.022\n",
      "epoch #311\tmean reward = 12.000\tepsilon = 0.022\n",
      "epoch #312\tmean reward = 15.000\tepsilon = 0.022\n",
      "epoch #313\tmean reward = 12.000\tepsilon = 0.022\n",
      "epoch #314\tmean reward = 22.000\tepsilon = 0.021\n",
      "epoch #315\tmean reward = 22.000\tepsilon = 0.021\n",
      "epoch #316\tmean reward = 45.000\tepsilon = 0.021\n",
      "epoch #317\tmean reward = 26.000\tepsilon = 0.021\n",
      "epoch #318\tmean reward = 10.000\tepsilon = 0.020\n",
      "epoch #319\tmean reward = 10.000\tepsilon = 0.020\n",
      "epoch #320\tmean reward = 9.000\tepsilon = 0.020\n",
      "epoch #321\tmean reward = 10.000\tepsilon = 0.020\n",
      "epoch #322\tmean reward = 8.000\tepsilon = 0.020\n",
      "epoch #323\tmean reward = 9.000\tepsilon = 0.019\n",
      "epoch #324\tmean reward = 9.000\tepsilon = 0.019\n",
      "epoch #325\tmean reward = 10.000\tepsilon = 0.019\n",
      "epoch #326\tmean reward = 10.000\tepsilon = 0.019\n",
      "epoch #327\tmean reward = 9.000\tepsilon = 0.019\n",
      "epoch #328\tmean reward = 8.000\tepsilon = 0.019\n",
      "epoch #329\tmean reward = 9.000\tepsilon = 0.018\n",
      "epoch #330\tmean reward = 8.000\tepsilon = 0.018\n",
      "epoch #331\tmean reward = 9.000\tepsilon = 0.018\n",
      "epoch #332\tmean reward = 9.000\tepsilon = 0.018\n",
      "epoch #333\tmean reward = 8.000\tepsilon = 0.018\n",
      "epoch #334\tmean reward = 8.000\tepsilon = 0.017\n",
      "epoch #335\tmean reward = 10.000\tepsilon = 0.017\n",
      "epoch #336\tmean reward = 10.000\tepsilon = 0.017\n",
      "epoch #337\tmean reward = 10.000\tepsilon = 0.017\n",
      "epoch #338\tmean reward = 10.000\tepsilon = 0.017\n",
      "epoch #339\tmean reward = 12.000\tepsilon = 0.017\n",
      "epoch #340\tmean reward = 8.000\tepsilon = 0.016\n",
      "epoch #341\tmean reward = 9.000\tepsilon = 0.016\n",
      "epoch #342\tmean reward = 10.000\tepsilon = 0.016\n",
      "epoch #343\tmean reward = 10.000\tepsilon = 0.016\n",
      "epoch #344\tmean reward = 8.000\tepsilon = 0.016\n",
      "epoch #345\tmean reward = 12.000\tepsilon = 0.016\n",
      "epoch #346\tmean reward = 9.000\tepsilon = 0.015\n",
      "epoch #347\tmean reward = 9.000\tepsilon = 0.015\n",
      "epoch #348\tmean reward = 10.000\tepsilon = 0.015\n",
      "epoch #349\tmean reward = 10.000\tepsilon = 0.015\n",
      "epoch #350\tmean reward = 9.000\tepsilon = 0.015\n",
      "epoch #351\tmean reward = 10.000\tepsilon = 0.015\n",
      "epoch #352\tmean reward = 8.000\tepsilon = 0.015\n",
      "epoch #353\tmean reward = 8.000\tepsilon = 0.014\n",
      "epoch #354\tmean reward = 10.000\tepsilon = 0.014\n",
      "epoch #355\tmean reward = 10.000\tepsilon = 0.014\n",
      "epoch #356\tmean reward = 9.000\tepsilon = 0.014\n",
      "epoch #357\tmean reward = 9.000\tepsilon = 0.014\n",
      "epoch #358\tmean reward = 10.000\tepsilon = 0.014\n",
      "epoch #359\tmean reward = 9.000\tepsilon = 0.014\n",
      "epoch #360\tmean reward = 9.000\tepsilon = 0.013\n",
      "epoch #361\tmean reward = 9.000\tepsilon = 0.013\n",
      "epoch #362\tmean reward = 9.000\tepsilon = 0.013\n",
      "epoch #363\tmean reward = 9.000\tepsilon = 0.013\n",
      "epoch #364\tmean reward = 11.000\tepsilon = 0.013\n",
      "epoch #365\tmean reward = 10.000\tepsilon = 0.013\n",
      "epoch #366\tmean reward = 9.000\tepsilon = 0.013\n",
      "epoch #367\tmean reward = 10.000\tepsilon = 0.013\n",
      "epoch #368\tmean reward = 10.000\tepsilon = 0.012\n",
      "epoch #369\tmean reward = 10.000\tepsilon = 0.012\n",
      "epoch #370\tmean reward = 11.000\tepsilon = 0.012\n",
      "epoch #371\tmean reward = 10.000\tepsilon = 0.012\n",
      "epoch #372\tmean reward = 11.000\tepsilon = 0.012\n",
      "epoch #373\tmean reward = 11.000\tepsilon = 0.012\n",
      "epoch #374\tmean reward = 13.000\tepsilon = 0.012\n",
      "epoch #375\tmean reward = 11.000\tepsilon = 0.012\n",
      "epoch #376\tmean reward = 15.000\tepsilon = 0.011\n",
      "epoch #377\tmean reward = 12.000\tepsilon = 0.011\n",
      "epoch #378\tmean reward = 14.000\tepsilon = 0.011\n",
      "epoch #379\tmean reward = 15.000\tepsilon = 0.011\n",
      "epoch #380\tmean reward = 15.000\tepsilon = 0.011\n",
      "epoch #381\tmean reward = 16.000\tepsilon = 0.011\n",
      "epoch #382\tmean reward = 31.000\tepsilon = 0.011\n",
      "epoch #383\tmean reward = 39.000\tepsilon = 0.011\n",
      "epoch #384\tmean reward = 102.000\tepsilon = 0.011\n",
      "epoch #385\tmean reward = 88.000\tepsilon = 0.010\n",
      "epoch #386\tmean reward = 54.000\tepsilon = 0.010\n",
      "epoch #387\tmean reward = 53.000\tepsilon = 0.010\n",
      "epoch #388\tmean reward = 46.000\tepsilon = 0.010\n",
      "epoch #389\tmean reward = 52.000\tepsilon = 0.010\n",
      "epoch #390\tmean reward = 383.000\tepsilon = 0.010\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "\n",
    "for i in range(1000):    \n",
    "    tot_rewards = [calculate_reward(env, t_max = 1000, epsilon = epsilon, train = True)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(tot_rewards), epsilon))\n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4\n",
    "\n",
    "    if np.mean(tot_rewards) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
